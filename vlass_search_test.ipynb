{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack Pope\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\Jack Pope\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "C:\\Users\\Jack Pope\\AppData\\Roaming\\Python\\Python39\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "from astropy.io import fits as pyfits\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "from astropy.table import Table\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.wcs import WCS\n",
    "from astropy.time import Time\n",
    "from contextlib import closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astropy.wcs import FITSFixedWarning\n",
    "\n",
    "# Suppress specific FITSFixedWarnings\n",
    "warnings.filterwarnings('ignore', category=FITSFixedWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Tiles Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded to: VLASS_dyn_summary.php\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://archive-new.nrao.edu/vlass/VLASS_dyn_summary.php'\n",
    "output_file = 'VLASS_dyn_summary.php'\n",
    "\n",
    "urllib.request.urlretrieve(url, output_file)\n",
    "\n",
    "print(f'File downloaded to: {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['T01t01', 'T01t01', 'T01t01', ..., 'T32t02', 'T32t02', 'T32t02'],\n",
       "       dtype='<U6'),\n",
       " array([-40., -40., -40., ...,  85.,  85.,  85.]),\n",
       " array([-36., -36., -36., ...,  90.,  90.,  90.]),\n",
       " array([ 0.,  0.,  0., ..., 12., 12., 12.]),\n",
       " array([ 0.5,  0.5,  0.5, ..., 24. , 24. , 24. ]),\n",
       " array(['VLASS1.1', 'VLASS2.1', 'VLASS3.1', ..., 'VLASS3.1', 'VLASS1.1',\n",
       "        'VLASS2.1'], dtype='<U8'),\n",
       " array(['2018-02-07', '2020-10-25', '2023-06-06', ..., '2023-04-24',\n",
       "        '2017-09-29', '2020-08-29'], dtype='<U13'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tiles():\n",
    "    \"\"\" Get tiles \n",
    "    I ran wget https://archive-new.nrao.edu/vlass/VLASS_dyn_summary.php\n",
    "    \"\"\"\n",
    "    fname = \"VLASS_dyn_summary.php\"\n",
    "    inputf = open(fname, \"r\")\n",
    "    lines = inputf.readlines()\n",
    "    inputf.close()\n",
    "\n",
    "    header = list(filter(None, lines[0].split(\"  \")))\n",
    "    # get rid of white spaces\n",
    "    header = np.array([val.strip() for val in header])\n",
    "\n",
    "    names = []\n",
    "    dec_min = []\n",
    "    dec_max = []\n",
    "    ra_min = []\n",
    "    ra_max = []\n",
    "    obsdate = []\n",
    "    epoch = []\n",
    "\n",
    "    # Starting at lines[3], read in values\n",
    "    for line in lines[3:]:\n",
    "        dat = list(filter(None, line.split(\"  \"))) \n",
    "        dat = np.array([val.strip() for val in dat]) \n",
    "        names.append(dat[0])\n",
    "        dec_min.append(float(dat[1]))\n",
    "        dec_max.append(float(dat[2]))\n",
    "        ra_min.append(float(dat[3]))\n",
    "        ra_max.append(float(dat[4]))\n",
    "        obsdate.append(dat[6])\n",
    "        epoch.append(dat[5])\n",
    "\n",
    "    names = np.array(names)\n",
    "    dec_min = np.array(dec_min)\n",
    "    dec_max = np.array(dec_max)\n",
    "    ra_min = np.array(ra_min)\n",
    "    ra_max = np.array(ra_max)\n",
    "    obsdate = np.array(obsdate)\n",
    "    epoch = np.array(epoch)\n",
    "\n",
    "    return (names, dec_min, dec_max, ra_min, ra_max, epoch, obsdate)\n",
    "\n",
    "\n",
    "get_tiles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Tiles Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tiles(tiles, c):\n",
    "    \"\"\" Now that you've processed the file, search for the given RA and Dec\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    c: SkyCoord object\n",
    "    \"\"\"\n",
    "    ra_h = c.ra.hour\n",
    "    dec_d = c.dec.deg\n",
    "    names, dec_min, dec_max, ra_min, ra_max, epochs, obsdate = tiles\n",
    "    has_dec = np.logical_and(dec_d > dec_min, dec_d < dec_max)\n",
    "    has_ra = np.logical_and(ra_h > ra_min, ra_h < ra_max)\n",
    "    in_tile = np.logical_and(has_ra, has_dec)\n",
    "    name = names[in_tile]\n",
    "    epoch = epochs[in_tile]\n",
    "    date = obsdate[in_tile]\n",
    "    if len(name) == 0:\n",
    "        print(\"Sorry, no tile found.\")\n",
    "        return None, None, None\n",
    "    else:\n",
    "        return name, epoch, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Subtiles Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtiles(tilename, epoch):\n",
    "    \"\"\" For a given tile name, get the filenames in the VLASS directory.\n",
    "    Parse those filenames and return a list of subtile RA and Dec.\n",
    "    RA and Dec returned as a SkyCoord object\n",
    "    \"\"\"\n",
    "    if epoch =='VLASS1.2':\n",
    "        epoch = 'VLASS1.2v2'\n",
    "    elif epoch =='VLASS1.1':\n",
    "        epoch = 'VLASS1.1v2'\n",
    "    url_full = 'https://archive-new.nrao.edu/vlass/quicklook/%s/%s/' %(epoch,tilename)\n",
    "    urlpath = urlopen(url_full)\n",
    "    # Get site HTML coding\n",
    "    string = (urlpath.read().decode('utf-8')).split(\"\\n\")\n",
    "    # clean the HTML elements of trailing and leading whitespace\n",
    "    vals = np.array([val.strip() for val in string])\n",
    "    # Make list of HTML link elements\n",
    "    keep_link = np.array([\"href\" in val.strip() for val in string])\n",
    "    # Make list of HTML elements with the tile name\n",
    "    keep_name = np.array([tilename in val.strip() for val in string])\n",
    "    # Cross reference the two lists above to keep only the HTML elements with the tile name and a link\n",
    "    string_keep = vals[np.logical_and(keep_link, keep_name)]\n",
    "    # Keep only the links from the HTML elements (they are the 7th element since 6 quote marks precede it)\n",
    "    fname = np.array([val.split(\"\\\"\")[7] for val in string_keep])\n",
    "    # Take out the element of the link that encodes the RA and declination\n",
    "    pos_raw = np.array([val.split(\".\")[4] for val in fname])\n",
    "    if '-' in pos_raw[0]:\n",
    "        # dec < 0\n",
    "        ra_raw = np.array([val.split(\"-\")[0] for val in pos_raw])\n",
    "        dec_raw = np.array([val.split(\"-\")[1] for val in pos_raw])\n",
    "    else:\n",
    "        # dec > 0\n",
    "        ra_raw = np.array([val.split(\"+\")[0] for val in pos_raw])\n",
    "        dec_raw = np.array([val.split(\"+\")[1] for val in pos_raw])\n",
    "    ra = []\n",
    "    dec = []\n",
    "    for ii,val in enumerate(ra_raw):\n",
    "        # 24 hours is the same as hour 0\n",
    "        if val[1:3] == '24':\n",
    "            rah = '00'\n",
    "        else:\n",
    "            rah = val[1:3]\n",
    "        # calculate RA in hours mins and seconds\n",
    "        hms = \"%sh%sm%ss\" %(rah, val[3:5], val[5:])\n",
    "        ra.append(hms)\n",
    "        # calculate Dec in degrees arcminutes and arcseconds\n",
    "        dms = \"%sd%sm%ss\" %(\n",
    "                dec_raw[ii][0:2], dec_raw[ii][2:4], dec_raw[ii][4:])\n",
    "        dec.append(dms)\n",
    "    ra = np.array(ra)\n",
    "    dec = np.array(dec)\n",
    "    c_tiles = SkyCoord(ra, dec, frame='icrs')\n",
    "    return fname, c_tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Numpy Cutout Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cutout(imname, name, c, epoch, save_dir, download_image = True):\n",
    "\n",
    "    # Position of source\n",
    "    ra_deg = c.ra.deg\n",
    "    dec_deg = c.dec.deg\n",
    "\n",
    "    print(\"Cutout centered at position %s, %s\" % (ra_deg, dec_deg))\n",
    "\n",
    "    try:\n",
    "        # Open image and establish coordinate system\n",
    "        with pyfits.open(imname) as hdul:\n",
    "            im = hdul[0].data[0, 0]\n",
    "            w = WCS(hdul[0].header)\n",
    "        \n",
    "            # Find the source position in pixels.\n",
    "            # This will be the center of our image.\n",
    "            src_pix = w.wcs_world2pix([[ra_deg, dec_deg, 0, 0]], 0)\n",
    "            x = src_pix[0, 0] + 1\n",
    "            y = src_pix[0, 1] + 1\n",
    "\n",
    "            # Check if the source is actually in the image\n",
    "            pix1 = hdul[0].header['CRPIX1']\n",
    "            pix2 = hdul[0].header['CRPIX2']\n",
    "            badx = np.logical_or(x < 0, x > 2 * pix1)\n",
    "            bady = np.logical_or(y < 0, y > 2 * pix2)\n",
    "            if np.logical_and(badx, bady):\n",
    "                print(\"Tile has not been imaged at the position of the source\")\n",
    "                return None\n",
    "            else:\n",
    "                # Set the dimensions of the image\n",
    "                # Say we want it to be 12 arcseconds on a side,\n",
    "                # to match the DES images\n",
    "                image_dim_arcsec = 12\n",
    "                delt1 = hdul[0].header['CDELT1']\n",
    "                delt2 = hdul[0].header['CDELT2']\n",
    "                cutout_size = image_dim_arcsec / 3600  # in degrees\n",
    "                dside1 = -cutout_size / 2. / delt1\n",
    "                dside2 = cutout_size / 2. / delt2\n",
    "\n",
    "                vmin = -1e-4\n",
    "                vmax = 1e-3\n",
    "\n",
    "                im_plot_raw = im[int(y - dside1):int(y + dside1), int(x - dside2):int(x + dside2)]\n",
    "                im_plot = np.ma.masked_invalid(im_plot_raw)\n",
    "\n",
    "                # 3-sigma clipping (find root mean square of values that are not above 3 standard deviations)\n",
    "                rms_temp = np.ma.std(im_plot)\n",
    "                keep = np.ma.abs(im_plot) <= 3 * rms_temp\n",
    "                rms = np.ma.std(im_plot[keep])\n",
    "\n",
    "                # Find peak flux in entire image\n",
    "                # Check if im_plot.flatten() is empty\n",
    "                if im_plot.flatten().size == 0:\n",
    "                    print(\"Tile has not been imaged at the position of the source\")\n",
    "                    return None\n",
    "                else:\n",
    "                    peak_flux = np.ma.max(im_plot.flatten())\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(6, 6))  # Create a square figure\n",
    "                ax.imshow(\n",
    "                    np.flipud(im_plot),\n",
    "                    extent=[-0.5 * cutout_size * 3600., 0.5 * cutout_size * 3600.,\n",
    "                            -0.5 * cutout_size * 3600., 0.5 * cutout_size * 3600],\n",
    "                    vmin=vmin, vmax=vmax, cmap='YlOrRd')\n",
    "\n",
    "                peakstr = \"Peak Flux %s mJy\" % (np.round(peak_flux * 1e3, 3))\n",
    "                rmsstr = \"RMS Flux %s mJy\" % (np.round(rms * 1e3, 3))\n",
    "\n",
    "                title_str = r'$\\bf{%s}$' % epoch + '\\n' + '%s: %s;\\n%s' % (name, peakstr, rmsstr)\n",
    "                ax.set_title(title_str, fontsize=10)\n",
    "                ax.set_xlabel(\"Offset in RA (arcsec)\")\n",
    "                ax.set_ylabel(\"Offset in Dec (arcsec)\")\n",
    "\n",
    "                ax.set_aspect('equal')  # Ensure the plot is square\n",
    "                ax.figure.tight_layout()  # Adjust layout to fit everything nicely\n",
    "\n",
    "                filename = f\"{name}_{epoch}.png\"\n",
    "                filepath = os.path.join(save_dir, filename)\n",
    "                plt.savefig(filepath)\n",
    "                plt.close(fig)\n",
    "\n",
    "                print(f\"PNG saved successfully: {filepath}\")\n",
    "\n",
    "        return peak_flux, rms, filepath\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {imname}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZTFID</th>\n",
       "      <th>IAUID</th>\n",
       "      <th>RA</th>\n",
       "      <th>Dec</th>\n",
       "      <th>peakt</th>\n",
       "      <th>peakfilt</th>\n",
       "      <th>peakmag</th>\n",
       "      <th>peakabs</th>\n",
       "      <th>duration</th>\n",
       "      <th>rise</th>\n",
       "      <th>fade</th>\n",
       "      <th>type</th>\n",
       "      <th>redshift</th>\n",
       "      <th>b</th>\n",
       "      <th>A_V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ZTF18aagtwyh</td>\n",
       "      <td>SN2021oud</td>\n",
       "      <td>12:39:41.92</td>\n",
       "      <td>+16:32:16.5</td>\n",
       "      <td>1384.70</td>\n",
       "      <td>r</td>\n",
       "      <td>18.3130</td>\n",
       "      <td>-19.04</td>\n",
       "      <td>&gt;26.011</td>\n",
       "      <td>14.991</td>\n",
       "      <td>&gt;11.02</td>\n",
       "      <td>SN Ia</td>\n",
       "      <td>0.06604</td>\n",
       "      <td>79.066215</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ZTF18aahatvc</td>\n",
       "      <td>SN2020itn</td>\n",
       "      <td>15:22:04.57</td>\n",
       "      <td>+30:02:55.5</td>\n",
       "      <td>985.78</td>\n",
       "      <td>r</td>\n",
       "      <td>18.1146</td>\n",
       "      <td>-19.59</td>\n",
       "      <td>34.591</td>\n",
       "      <td>10.633</td>\n",
       "      <td>23.958</td>\n",
       "      <td>SN Ia-91T</td>\n",
       "      <td>0.07780</td>\n",
       "      <td>56.950348</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ZTF18aahesrp</td>\n",
       "      <td>SN2018aqy</td>\n",
       "      <td>08:35:45.43</td>\n",
       "      <td>+28:16:12.9</td>\n",
       "      <td>217.69</td>\n",
       "      <td>r</td>\n",
       "      <td>18.5448</td>\n",
       "      <td>-18.97</td>\n",
       "      <td>&gt;22.118</td>\n",
       "      <td>&gt;3</td>\n",
       "      <td>19.118</td>\n",
       "      <td>SN Ia</td>\n",
       "      <td>0.07000</td>\n",
       "      <td>34.094576</td>\n",
       "      <td>0.111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ZTFID      IAUID           RA          Dec    peakt peakfilt  \\\n",
       "20  ZTF18aagtwyh  SN2021oud  12:39:41.92  +16:32:16.5  1384.70        r   \n",
       "21  ZTF18aahatvc  SN2020itn  15:22:04.57  +30:02:55.5   985.78        r   \n",
       "22  ZTF18aahesrp  SN2018aqy  08:35:45.43  +28:16:12.9   217.69        r   \n",
       "\n",
       "    peakmag peakabs duration    rise    fade       type  redshift          b  \\\n",
       "20  18.3130  -19.04  >26.011  14.991  >11.02      SN Ia   0.06604  79.066215   \n",
       "21  18.1146  -19.59   34.591  10.633  23.958  SN Ia-91T   0.07780  56.950348   \n",
       "22  18.5448  -18.97  >22.118      >3  19.118      SN Ia   0.07000  34.094576   \n",
       "\n",
       "      A_V  \n",
       "20  0.067  \n",
       "21  0.057  \n",
       "22  0.111  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "\n",
    "\n",
    "# Example coordinates\n",
    "coord_str = \"13:14:25.45 +50:58:39.7\"\n",
    "c1 = SkyCoord(coord_str, unit=(u.hourangle, u.deg))\n",
    "\n",
    "name = \"ZTF18aagrtxs\"\n",
    "\n",
    "\n",
    "coord_str2 = \"10:35:32.09 +37:38:59.0\"\n",
    "name = \"ZTF17aabtvsy\"\n",
    "c2 = SkyCoord(coord_str2, unit=(u.hourangle, u.deg))\n",
    "\n",
    "\n",
    "file_path = 'ia_supernovae_stuff/ia_full_data/ia_supernovae.csv'\n",
    "ia_supernovae_df = pd.read_csv(file_path)\n",
    "test_df = ia_supernovae_df.iloc[20:23]\n",
    "\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple objects to Event List\n",
    "and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'ZTF18aagtwyh',\n",
       "  'Skycoord': <SkyCoord (ICRS): (ra, dec) in deg\n",
       "      (189.92466667, 16.53791667)>},\n",
       " {'Name': 'ZTF18aahatvc',\n",
       "  'Skycoord': <SkyCoord (ICRS): (ra, dec) in deg\n",
       "      (230.51904167, 30.04875)>},\n",
       " {'Name': 'ZTF18aahesrp',\n",
       "  'Skycoord': <SkyCoord (ICRS): (ra, dec) in deg\n",
       "      (128.93929167, 28.27025)>}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataframe_to_objects(dataframe, name_id_col = 'ZTFID', ra_id_col = 'RA', dec_id_col = 'Dec'):\n",
    "\n",
    "    new_dataframe = []\n",
    "    for i, object in dataframe.iterrows():\n",
    "        name = object[name_id_col]\n",
    "        coord_str = object[ra_id_col] + object[dec_id_col]\n",
    "        c = SkyCoord(coord_str, unit=(u.hourangle, u.deg))\n",
    "        object_data = {'Name': name,\n",
    "                       'Skycoord': c}\n",
    "        new_dataframe.append(object_data)\n",
    "    return new_dataframe\n",
    "\n",
    "dataframe_to_objects(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tiles_and_sort(c):\n",
    "    \"\"\"\n",
    "    Gets tiles and sorts the available epochs in order, returning tiles, epochs, and their observed dates\n",
    "    \"\"\"\n",
    "\n",
    "    tiles = get_tiles()\n",
    "    tilenames, epochs, obsdates = search_tiles(tiles, c)\n",
    "\n",
    "    # Sort the tiles by the epochs so most recent goes last\n",
    "    combined = list(zip(tilenames, epochs, obsdates))\n",
    "    combined_sorted = sorted(combined, key=lambda x: x[1])\n",
    "    tilenames, epochs, obsdates = zip(*combined_sorted)\n",
    "\n",
    "    return tilenames, epochs, obsdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_observed_tiles(tilenames):\n",
    "    \"\"\"\n",
    "    Checks if there is a list of tilenames (from search_tiles_and_sort) or if it is just None\n",
    "\n",
    "    Returns True if there are tiles\n",
    "    Returns False if there are none\n",
    "    \"\"\"\n",
    "    if tilenames[0] is None:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_to_multiple_epochs(name, c):\n",
    "    \"\"\"\n",
    "    Takes the list of lists from search_tiles_and_sort and makes them into separate dictionaries,\n",
    "    each representing a unique epoch-date observed\n",
    "    \"\"\"\n",
    "\n",
    "    object_lists = []\n",
    "    set_list = search_tiles_and_sort(c)\n",
    "    # get the length of the individual lists (how many epochs show up)\n",
    "    for x in range(len(set_list[0])):\n",
    "        obs = {'Name': name,\n",
    "               'c': c,\n",
    "               'Tile': set_list[0][x],\n",
    "               'Epoch': set_list[1][x],\n",
    "               'Date': set_list[2][x]}\n",
    "        object_lists.append(obs)\n",
    "\n",
    "    return object_lists\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_objects_to_event_list(dataframe, name_id_col = 'ZTFID', ra_id_col = 'RA', dec_id_col = 'Dec'):\n",
    "    \n",
    "    \"\"\" Given a pandas dataframe with name, ra, and dec, returns the a list of dictionaries\n",
    "    which each have the name, skycoord, tile, epoch, and date of observation\n",
    "    \"\"\"\n",
    "\n",
    "    final_object_epoch_list = []\n",
    "    list_dict = dataframe_to_objects(dataframe, name_id_col, ra_id_col, dec_id_col)\n",
    "    for obj in list_dict:\n",
    "        name = obj['Name']\n",
    "        c = obj['Skycoord']\n",
    "        list_epoch_tiles = object_to_multiple_epochs(name, c)\n",
    "        for x in list_epoch_tiles:\n",
    "            final_object_epoch_list.append(x)\n",
    "\n",
    "    return final_object_epoch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_observed_tiles(most_recent_epoch = \"VLASS3.2\"):\n",
    "    \"\"\"\n",
    "    Get all tiles that have been observed and put them into a list\n",
    "    \"\"\"\n",
    "\n",
    "    url_full = 'https://archive-new.nrao.edu/vlass/quicklook/%s/' %(most_recent_epoch)\n",
    "    with closing(urlopen(url_full)) as urlpath:\n",
    "        # Get site HTML coding\n",
    "        string = (urlpath.read().decode('utf-8')).split(\"\\n\")\n",
    "        # clean the HTML elements of trailing and leading whitespace\n",
    "        vals = np.array([val.strip() for val in string])\n",
    "        # Make list of useful html elements\n",
    "        files = np.array(['alt=\"[DIR]\"' in val.strip() for val in string])\n",
    "        useful = vals[files]\n",
    "        # Splice out the name from the link\n",
    "        obsname = np.array([val.split(\"\\\"\")[7] for val in useful])\n",
    "        observed_tiles_current_epoch = np.char.replace(obsname, '/', '')\n",
    "        \n",
    "    return observed_tiles_current_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_obsdate(obsdate):\n",
    "    \"\"\"\n",
    "    Checks an observed date and converts it to the proper date format (iso)\n",
    "        if it is not \"Not Submitted\" or \"Scheduled\n",
    "    \n",
    "    \"\"\"\n",
    "    # first check that the date isn't a special description\n",
    "    if obsdate not in ['Scheduled', 'Not submitted']:\n",
    "        try:\n",
    "            obsdate = Time(obsdate, format='iso')\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid date format: {obsdate}. Error: {e}\")\n",
    "            obsdate = 'Invalid date'\n",
    "    # if it is, just say it's an invalid date\n",
    "    else:\n",
    "        obsdate = f\"Invalid Date: '{obsdate}'\"\n",
    "\n",
    "    return obsdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_epoch(epoch):\n",
    "    \"\"\"\n",
    "    Changes the name of the epoch from VLASS1.1 and VLASS1.2 to VLASS1.1v2 and 1.2v2 respectively\n",
    "    \"\"\"\n",
    "    # adjust epoch names to properly match\n",
    "    if epoch=='VLASS1.2':\n",
    "        epoch = 'VLASS1.2v2'\n",
    "    elif epoch =='VLASS1.1':\n",
    "        epoch = 'VLASS1.1v2'\n",
    "\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_epoch_observed(epoch, tilename, observed_tiles_current_epoch):\n",
    "    \"\"\"\n",
    "    Checks if the tile is observed based on what epoch we're in\n",
    "    \"\"\"\n",
    "    # define past and present epochs\n",
    "    past_epochs = [\"VLASS1.1v2\", \"VLASS1.2v2\", \"VLASS2.1\", \"VLASS2.2\", \"VLASS3.1\"]\n",
    "    current_epoch = \"VLASS3.2\"\n",
    "\n",
    "    # if it's a past epoch, we know it was observed\n",
    "    if epoch in past_epochs:\n",
    "        observed = True\n",
    "    else:\n",
    "        # if it's in current epoch, we need to check if it's been observed yet\n",
    "        if epoch == current_epoch:\n",
    "            # check if tile has been observed\n",
    "            if tilename in observed_tiles_current_epoch:\n",
    "                observed = True\n",
    "        # if different epoch or hasn't been observed, set observed to False\n",
    "        else:\n",
    "            observed = False\n",
    "\n",
    "    return observed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_check_subtile(c, epoch, tilename):\n",
    "    \"\"\"\n",
    "    If the tile was assumed to be observed, get the nearest subtile to the coordinates of the object\n",
    "    \"\"\"\n",
    "    subtiles, c_tiles = get_subtiles(tilename, epoch)\n",
    "    # check that there are actually some subtiles\n",
    "    if c_tiles == []:\n",
    "        subtile = None\n",
    "    else:\n",
    "        # Find angular separation from the tiles to the location\n",
    "        dist = c.separation(c_tiles)\n",
    "        # Find tile with the smallest separation \n",
    "        subtile = subtiles[np.argmin(dist)]\n",
    "\n",
    "    return subtile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_and_url_getter(subtile, epoch, tilename):\n",
    "    \"\"\"\n",
    "    Given a subtile, return the NRAO url\n",
    "    \"\"\"\n",
    "    fits_url = \"https://archive-new.nrao.edu/vlass/quicklook/%s/%s/%s\" %(\n",
    "            epoch, tilename, subtile)\n",
    "    fitsname = \"%s.I.iter1.image.pbcor.tt0.subim.fits\" %subtile[0:-1]\n",
    "\n",
    "    return fits_url, fitsname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def get_fits(fits_url, fitsname, fits_dir=\"ia_fits\"):\n",
    "    \"\"\"\n",
    "    Given the fits_url and fitsname, checks if the FITS file is downloaded in fits_dir\n",
    "    and downloads it if it hasn't been already.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(fits_dir):\n",
    "        os.makedirs(fits_dir)\n",
    "\n",
    "    # Define the local path for the FITS file\n",
    "    local_fits_path = os.path.join(fits_dir, fitsname)\n",
    "\n",
    "    # Check if FITS file has already been downloaded\n",
    "    if os.path.exists(local_fits_path):\n",
    "        print(f\"{local_fits_path} already exists. Skipping download.\")\n",
    "        return local_fits_path\n",
    "\n",
    "    # Define the full URL for the FITS file\n",
    "    full_fits_url = fits_url + fitsname\n",
    "\n",
    "    # Attempt to download the FITS file\n",
    "    try:\n",
    "        print(f\"Downloading {fitsname}...\")\n",
    "        subprocess.run(\n",
    "            [\"curl\", \"-o\", local_fits_path, full_fits_url],\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "        # Confirm if the file was downloaded successfully\n",
    "        if os.path.exists(local_fits_path):\n",
    "            print(f\"Downloaded {fitsname} successfully.\")\n",
    "            return local_fits_path\n",
    "        else:\n",
    "            print(f\"Failed to download {fitsname}. File not found after download attempt.\")\n",
    "            return None\n",
    "\n",
    "    except subprocess.CalledProcessError:\n",
    "        # Handle any errors from the curl command\n",
    "        print(f\"Error downloading {fitsname}. Check the URL or network connection.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_array(local_fits_path, c):\n",
    "    \"\"\"\n",
    "    Given a fits path, gets the array centered around the skycoords c\n",
    "    \"\"\"\n",
    "\n",
    "    # Position of source\n",
    "    ra_deg = c.ra.deg\n",
    "    dec_deg = c.dec.deg\n",
    "\n",
    "    try:\n",
    "        # Open image and establish coordinate system\n",
    "        with pyfits.open(local_fits_path) as hdul:\n",
    "            im = hdul[0].data[0, 0]\n",
    "            w = WCS(hdul[0].header)\n",
    "        \n",
    "            # Find the source position in pixels.\n",
    "            # This will be the center of our image.\n",
    "            src_pix = w.wcs_world2pix([[ra_deg, dec_deg, 0, 0]], 0)\n",
    "            x = src_pix[0, 0] + 1\n",
    "            y = src_pix[0, 1] + 1\n",
    "\n",
    "            # Check if the source is actually in the image\n",
    "            pix1 = hdul[0].header['CRPIX1']\n",
    "            pix2 = hdul[0].header['CRPIX2']\n",
    "            badx = np.logical_or(x < 0, x > 2 * pix1)\n",
    "            bady = np.logical_or(y < 0, y > 2 * pix2)\n",
    "            if np.logical_and(badx, bady):\n",
    "                print(\"Tile has not been imaged at the position of the source\")\n",
    "                return None\n",
    "            else:\n",
    "                # Set the dimensions of the image\n",
    "                # Say we want it to be 12 arcseconds on a side,\n",
    "                # to match the DES images\n",
    "                image_dim_arcsec = 12\n",
    "                delt1 = hdul[0].header['CDELT1']\n",
    "                delt2 = hdul[0].header['CDELT2']\n",
    "                cutout_size = image_dim_arcsec / 3600  # in degrees\n",
    "                dside1 = -cutout_size / 2. / delt1\n",
    "                dside2 = cutout_size / 2. / delt2\n",
    "\n",
    "                im_plot_raw = im[int(y - dside1):int(y + dside1), int(x - dside2):int(x + dside2)]\n",
    "                im_plot = np.ma.masked_invalid(im_plot_raw)\n",
    "\n",
    "                return im_plot\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {local_fits_path}: {e}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'j' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36944\\3942822043.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m \u001b[0mcheck_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pluh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pluhhhh'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'j' is not defined"
     ]
    }
   ],
   "source": [
    "def check_image(local_fits_path, name, c, epoch):\n",
    "\n",
    "    # Position of source\n",
    "    ra_deg = c.ra.deg\n",
    "    dec_deg = c.dec.deg\n",
    "\n",
    "    print(\"Cutout centered at position %s, %s\" % (ra_deg, dec_deg))\n",
    "\n",
    "    try:\n",
    "        # Open image and establish coordinate system\n",
    "        with pyfits.open(local_fits_path) as hdul:\n",
    "            im = hdul[0].data[0, 0]\n",
    "            w = WCS(hdul[0].header)\n",
    "        \n",
    "            # Find the source position in pixels.\n",
    "            # This will be the center of our image.\n",
    "            src_pix = w.wcs_world2pix([[ra_deg, dec_deg, 0, 0]], 0)\n",
    "            x = src_pix[0, 0] + 1\n",
    "            y = src_pix[0, 1] + 1\n",
    "\n",
    "            # Check if the source is actually in the image\n",
    "            pix1 = hdul[0].header['CRPIX1']\n",
    "            pix2 = hdul[0].header['CRPIX2']\n",
    "            badx = np.logical_or(x < 0, x > 2 * pix1)\n",
    "            bady = np.logical_or(y < 0, y > 2 * pix2)\n",
    "            if np.logical_and(badx, bady):\n",
    "                print(\"Tile has not been imaged at the position of the source\")\n",
    "                return None\n",
    "            else:\n",
    "                # Set the dimensions of the image\n",
    "                # Say we want it to be 12 arcseconds on a side,\n",
    "                # to match the DES images\n",
    "                image_dim_arcsec = 12\n",
    "                delt1 = hdul[0].header['CDELT1']\n",
    "                delt2 = hdul[0].header['CDELT2']\n",
    "                cutout_size = image_dim_arcsec / 3600  # in degrees\n",
    "                dside1 = -cutout_size / 2. / delt1\n",
    "                dside2 = cutout_size / 2. / delt2\n",
    "\n",
    "                vmin = -1e-4\n",
    "                vmax = 1e-3\n",
    "\n",
    "                im_plot_raw = im[int(y - dside1):int(y + dside1), int(x - dside2):int(x + dside2)]\n",
    "                im_plot = np.ma.masked_invalid(im_plot_raw)\n",
    "\n",
    "                # 3-sigma clipping (find root mean square of values that are not above 3 standard deviations)\n",
    "                rms_temp = np.ma.std(im_plot)\n",
    "                keep = np.ma.abs(im_plot) <= 3 * rms_temp\n",
    "                rms = np.ma.std(im_plot[keep])\n",
    "\n",
    "                # Find peak flux in entire image\n",
    "                # Check if im_plot.flatten() is empty\n",
    "                if im_plot.flatten().size == 0:\n",
    "                    print(\"Tile has not been imaged at the position of the source\")\n",
    "                    return None\n",
    "                else:\n",
    "                    peak_flux = np.ma.max(im_plot.flatten())\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(6, 6))  # Create a square figure\n",
    "                ax.imshow(\n",
    "                    np.flipud(im_plot),\n",
    "                    extent=[-0.5 * cutout_size * 3600., 0.5 * cutout_size * 3600.,\n",
    "                            -0.5 * cutout_size * 3600., 0.5 * cutout_size * 3600],\n",
    "                    vmin=vmin, vmax=vmax, cmap='YlOrRd')\n",
    "\n",
    "                peakstr = \"Peak Flux %s mJy\" % (np.round(peak_flux * 1e3, 3))\n",
    "                rmsstr = \"RMS Flux %s mJy\" % (np.round(rms * 1e3, 3))\n",
    "\n",
    "                title_str = r'$\\bf{%s}$' % epoch + '\\n' + '%s: %s;\\n%s' % (name, peakstr, rmsstr)\n",
    "                ax.set_title(title_str, fontsize=10)\n",
    "                ax.set_xlabel(\"Offset in RA (arcsec)\")\n",
    "                ax.set_ylabel(\"Offset in Dec (arcsec)\")\n",
    "\n",
    "                ax.set_aspect('equal')  # Ensure the plot is square\n",
    "                ax.figure.tight_layout()  # Adjust layout to fit everything nicely\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {local_fits_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "check_image(j, 'pluh', c2, 'pluhhhh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def save_npy_array(name, epoch, array, save_dir = 'ia_npy_arrays'):\n",
    "\n",
    "    # Convert masked array to a regular ndarray (removes the mask)\n",
    "    array = array.filled(np.nan)  # Replace masked values with NaN (or any value you prefer)\n",
    "\n",
    "    # Make sure folder exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Define name of the file\n",
    "    file_path = os.path.join(save_dir, f\"{name}_{epoch}_array.npy\")\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File {file_path} already exists. Skipping save.\")\n",
    "        return file_path # Exit the function if the file exists\n",
    "\n",
    "    # Save the array as a .npy to the folder\n",
    "    np.save(file_path, array)\n",
    "    \n",
    "    print(f\"Array saved to {file_path}\")\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_to_npy(name, c, tile, epoch):\n",
    "    \"\"\" \n",
    "    Converts a single event, given name, skycoord, the tile, epoch, and date\n",
    "    (individual objects from multiple_objects_to_events)\n",
    "    and saves the fits and the array, returning the file path for the array\n",
    "    \"\"\"\n",
    "    ### MAKE SURE TO CHECK IF TILE EXISTSSSS\n",
    "    \n",
    "    # check if tile was observed\n",
    "    curr_obs_tiles = pull_observed_tiles()\n",
    "    epoch = adjust_epoch(epoch)\n",
    "    observed = check_epoch_observed(epoch, tile, curr_obs_tiles)\n",
    "\n",
    "    # if observed, check subtiles\n",
    "    if observed:\n",
    "        subtile = get_and_check_subtile(c, epoch, tile)\n",
    "        # if subtile exists, save the array\n",
    "        if subtile is not None:\n",
    "            fits_url, fitsname = image_and_url_getter(subtile, epoch, tile)\n",
    "            local_fits_path = get_fits(fits_url, fitsname, fits_dir=\"ia_fits\")\n",
    "            array = get_numpy_array(local_fits_path, c)\n",
    "            file_path = save_npy_array(name, epoch, array, save_dir = 'ia_npy_arrays')\n",
    "            return file_path\n",
    "        # if subtile doesn't exist, return NA\n",
    "        else:\n",
    "            print('Subtile does not exist')\n",
    "            return 'NA'\n",
    "    else:\n",
    "        print('Object not observed')\n",
    "        return 'NA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(columns=['Name', 'Ra_hourangle', 'Dec_degree', 'Epoch', 'Date', 'File_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Ra_hourangle</th>\n",
       "      <th>Dec_degree</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Date</th>\n",
       "      <th>File_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, Ra_hourangle, Dec_degree, Epoch, Date, File_path]\n",
       "Index: []"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ia_fits\\VLASS1.2.ql.T15t17.J123931+163000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits already exists. Skipping download.\n",
      "File ia_npy_arrays\\ZTF18aagtwyh_VLASS1.2v2_array.npy already exists. Skipping save.\n",
      "ia_fits\\VLASS2.2.ql.T15t17.J123931+163000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits already exists. Skipping download.\n",
      "File ia_npy_arrays\\ZTF18aagtwyh_VLASS2.2_array.npy already exists. Skipping save.\n",
      "ia_fits\\VLASS3.2.ql.T15t17.J123931+163000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits already exists. Skipping download.\n",
      "File ia_npy_arrays\\ZTF18aagtwyh_VLASS3.2_array.npy already exists. Skipping save.\n",
      "ia_fits\\VLASS1.1.ql.T18t21.J152047+303000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits already exists. Skipping download.\n",
      "File ia_npy_arrays\\ZTF18aahatvc_VLASS1.1v2_array.npy already exists. Skipping save.\n",
      "ia_fits\\VLASS2.1.ql.T18t21.J152047+303000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits already exists. Skipping download.\n",
      "File ia_npy_arrays\\ZTF18aahatvc_VLASS2.1_array.npy already exists. Skipping save.\n",
      "ia_fits\\VLASS3.1.ql.T18t21.J152047+303000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits already exists. Skipping download.\n",
      "File ia_npy_arrays\\ZTF18aahatvc_VLASS3.1_array.npy already exists. Skipping save.\n",
      "ia_fits\\VLASS1.2.ql.T18t12.J083523+283000.10.2048.v2.I.iter1.image.pbcor.tt0.subim.fits already exists. Skipping download.\n",
      "File ia_npy_arrays\\ZTF18aahesrp_VLASS1.2v2_array.npy already exists. Skipping save.\n",
      "ia_fits\\VLASS2.2.ql.T18t12.J083523+283000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits already exists. Skipping download.\n",
      "File ia_npy_arrays\\ZTF18aahesrp_VLASS2.2_array.npy already exists. Skipping save.\n",
      "ia_fits\\VLASS3.2.ql.T18t12.J083523+283000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits already exists. Skipping download.\n",
      "File ia_npy_arrays\\ZTF18aahesrp_VLASS3.2_array.npy already exists. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "def objects_to_df(in_dataframe, out_dataframe):\n",
    "    \"\"\"\n",
    "    Takes a dataframe from multiple_objects_to_event_list and converts gets the\n",
    "    array, saving the file_path and other information in a pandas dataframe\n",
    "\n",
    "    out_dataframe must be a dataframe with columns 'name', 'c', 'tile', 'epoch', 'date', 'file_path'\n",
    "    \"\"\"\n",
    "\n",
    "    for event in in_dataframe:\n",
    "        name = event['Name']\n",
    "        c = event['c']\n",
    "        tile = event['Tile']\n",
    "        epoch = event['Epoch']\n",
    "        event['File_path'] = event_to_npy(name, c, tile, epoch)\n",
    "        # convert dictionary to pd row\n",
    "        event_df = pd.DataFrame([event])\n",
    "        # append pandas row to the out_dataframe\n",
    "        out_dataframe = pd.concat([out_dataframe, event_df], ignore_index=True)\n",
    "    return out_dataframe\n",
    "\n",
    "out_df = objects_to_df(multiple_objects_to_event_list(test_df), out_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'ia_supernovae_stuff/ia_full_data/ia_supernovae.csv'\n",
    "ia_supernovae_df = pd.read_csv(file_path)\n",
    "test_df = ia_supernovae_df.iloc[0:23]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objects_to_df(in_dataframe, out_dataframe):\n",
    "    \"\"\"\n",
    "    Takes a dataframe from multiple_objects_to_event_list and converts it to a\n",
    "    pandas DataFrame, saving file paths and other information.\n",
    "\n",
    "    out_dataframe must be a DataFrame with columns 'Name', 'Ra_hourangle', \n",
    "    'Dec_degree', 'Epoch', 'Date', 'File_path'.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_total = time.time()\n",
    "    rows = []  # Collect rows here for efficiency\n",
    "    \n",
    "    for event in in_dataframe:\n",
    "        start_event = time.time()\n",
    "\n",
    "        # Extract fields\n",
    "        name = event['Name']\n",
    "        c = event['c']\n",
    "        ra_hourangle = c.ra.hourangle\n",
    "        dec_degree = c.dec.degree\n",
    "        tile = event['Tile']\n",
    "        epoch = event['Epoch']\n",
    "        date = event['Date']\n",
    "        \n",
    "        # Generate file path\n",
    "        start_file_path = time.time()\n",
    "        file_path = event_to_npy(name, c, tile, epoch)\n",
    "        print(f\"Time for file path generation: {time.time() - start_file_path:.4f} seconds\")\n",
    "\n",
    "        # Add the row to the list\n",
    "        rows.append({\n",
    "            'Name': name,\n",
    "            'Ra_hourangle': ra_hourangle,\n",
    "            'Dec_degree': dec_degree,\n",
    "            'Epoch': epoch,\n",
    "            'Date': date,\n",
    "            'File_path': file_path\n",
    "        })\n",
    "        \n",
    "        # Total time for this event\n",
    "        print(f\"Time for processing event: {time.time() - start_event:.4f} seconds\\n\")\n",
    "\n",
    "    # Convert rows to a DataFrame\n",
    "    new_rows_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Check if out_dataframe is empty\n",
    "    if out_dataframe.empty:\n",
    "        out_dataframe = new_rows_df\n",
    "    else:\n",
    "        out_dataframe = pd.concat([out_dataframe, new_rows_df], ignore_index=True)\n",
    "\n",
    "    out_dataframe.to_csv(\"ia_supernovae_with_file_path.csv\", index=False)\n",
    "    print(\"Dataframe written to: ia_supernovae_with_file_path.csv\")\n",
    "\n",
    "    # Total time for all events\n",
    "    print(f\"Total time for all events: {time.time() - start_total:.4f} seconds\")\n",
    "    return out_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def update_csv(start_row, end_row, csv_name=\"ia_supernovae_with_file_path.csv\",\n",
    "                file_path = \"ia_supernovae_stuff/ia_full_data/ia_supernovae.csv\"):\n",
    "    \"\"\"\n",
    "    Updates or initializes a CSV file with processed data from a subset of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - start_row: Starting row index for the subset\n",
    "    - end_row: Ending row index for the subset\n",
    "    - csv_name: Name of the output CSV file to update or create\n",
    "    - file_name: Name of the file that contains the list of the transients being looked at\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the original DataFrame\n",
    "    ia_supernovae_df = pd.read_csv(file_path)\n",
    "    test_df = ia_supernovae_df.iloc[start_row:end_row]\n",
    "    \n",
    "    # Check if the output CSV exists\n",
    "    if os.path.exists(csv_name):\n",
    "        # If it exists, load the existing DataFrame\n",
    "        out_df = pd.read_csv(csv_name)\n",
    "    else:\n",
    "        # Initialize the DataFrame if the file doesn't exist\n",
    "        out_df = pd.DataFrame(columns=['Name', 'Ra_hourangle', 'Dec_degree', 'Epoch', 'Date', 'File_path'])\n",
    "\n",
    "    # Process and append rows\n",
    "    out_df = objects_to_df(multiple_objects_to_event_list(test_df), out_df)\n",
    "    \n",
    "    # Save back to the CSV\n",
    "    out_df.to_csv(csv_name, index=False)\n",
    "    print(f\"CSV updated: {csv_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading VLASS1.1.ql.T19t20.J151914+343000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS1.1.ql.T19t20.J151914+343000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aamvfeb_VLASS1.1v2_array.npy\n",
      "Time for file path generation: 5.3858 seconds\n",
      "Time for processing event: 5.3858 seconds\n",
      "\n",
      "Downloading VLASS2.1.ql.T19t20.J151914+343000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS2.1.ql.T19t20.J151914+343000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aamvfeb_VLASS2.1_array.npy\n",
      "Time for file path generation: 12.5197 seconds\n",
      "Time for processing event: 12.5197 seconds\n",
      "\n",
      "Downloading VLASS3.1.ql.T19t20.J151914+343000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS3.1.ql.T19t20.J151914+343000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aamvfeb_VLASS3.1_array.npy\n",
      "Time for file path generation: 11.5071 seconds\n",
      "Time for processing event: 11.5071 seconds\n",
      "\n",
      "Downloading VLASS1.1.ql.T26t11.J141716+633000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS1.1.ql.T26t11.J141716+633000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aamxads_VLASS1.1v2_array.npy\n",
      "Time for file path generation: 13.3107 seconds\n",
      "Time for processing event: 13.3107 seconds\n",
      "\n",
      "Downloading VLASS2.1.ql.T26t11.J141716+633000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS2.1.ql.T26t11.J141716+633000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aamxads_VLASS2.1_array.npy\n",
      "Time for file path generation: 3.5559 seconds\n",
      "Time for processing event: 3.5559 seconds\n",
      "\n",
      "Downloading VLASS3.1.ql.T26t11.J141716+633000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS3.1.ql.T26t11.J141716+633000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aamxads_VLASS3.1_array.npy\n",
      "Time for file path generation: 3.5191 seconds\n",
      "Time for processing event: 3.5191 seconds\n",
      "\n",
      "Downloading VLASS1.2.ql.T21t15.J145630+423000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS1.2.ql.T21t15.J145630+423000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aamzgzi_VLASS1.2v2_array.npy\n",
      "Time for file path generation: 3.8607 seconds\n",
      "Time for processing event: 3.8607 seconds\n",
      "\n",
      "Downloading VLASS2.2.ql.T21t15.J145630+423000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS2.2.ql.T21t15.J145630+423000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aamzgzi_VLASS2.2_array.npy\n",
      "Time for file path generation: 11.4657 seconds\n",
      "Time for processing event: 11.4657 seconds\n",
      "\n",
      "Downloading VLASS3.2.ql.T21t15.J145630+423000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS3.2.ql.T21t15.J145630+423000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aamzgzi_VLASS3.2_array.npy\n",
      "Time for file path generation: 3.8234 seconds\n",
      "Time for processing event: 3.8234 seconds\n",
      "\n",
      "Downloading VLASS1.1.ql.T19t22.J165947+323000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS1.1.ql.T19t22.J165947+323000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aangpkx_VLASS1.1v2_array.npy\n",
      "Time for file path generation: 14.5153 seconds\n",
      "Time for processing event: 14.5153 seconds\n",
      "\n",
      "Downloading VLASS2.1.ql.T19t22.J165947+323000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS2.1.ql.T19t22.J165947+323000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aangpkx_VLASS2.1_array.npy\n",
      "Time for file path generation: 3.7538 seconds\n",
      "Time for processing event: 3.7538 seconds\n",
      "\n",
      "Downloading VLASS3.1.ql.T19t22.J165947+323000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS3.1.ql.T19t22.J165947+323000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aangpkx_VLASS3.1_array.npy\n",
      "Time for file path generation: 13.9330 seconds\n",
      "Time for processing event: 13.9330 seconds\n",
      "\n",
      "Downloading VLASS1.1.ql.T24t18.J173633+533000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS1.1.ql.T24t18.J173633+533000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aanhpii_VLASS1.1v2_array.npy\n",
      "Time for file path generation: 4.2643 seconds\n",
      "Time for processing event: 4.2643 seconds\n",
      "\n",
      "Downloading VLASS2.1.ql.T24t18.J173633+533000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS2.1.ql.T24t18.J173633+533000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aanhpii_VLASS2.1_array.npy\n",
      "Time for file path generation: 3.4769 seconds\n",
      "Time for processing event: 3.4769 seconds\n",
      "\n",
      "Downloading VLASS3.1.ql.T24t18.J173633+533000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits...\n",
      "Downloaded VLASS3.1.ql.T24t18.J173633+533000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits successfully.\n",
      "Array saved to ia_npy_arrays\\ZTF18aanhpii_VLASS3.1_array.npy\n",
      "Time for file path generation: 3.7139 seconds\n",
      "Time for processing event: 3.7139 seconds\n",
      "\n",
      "Total time for all events: 112.6574 seconds\n",
      "CSV updated: ia_supernovae_with_file_path.csv\n"
     ]
    }
   ],
   "source": [
    "update_csv(65, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[{'Name': 'ZTF17aabtvsy',\n",
    "  'c': <SkyCoord (ICRS): (ra, dec) in deg\n",
    "      (158.88370833, 37.64972222)>,\n",
    "  'Tile': 'T20t14',\n",
    "  'Epoch': 'VLASS1.2',\n",
    "  'Date': '2019-05-03'},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi_ra = \"12h10m01.32s\"\n",
    "maxi_dec = \"+49d56m47.006s\"\n",
    "maxi = SkyCoord(ra = maxi_ra, dec = maxi_dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_search(name, c, date=None):\n",
    "    \"\"\" \n",
    "    Searches the VLASS catalog for a source\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    names: name of the sources\n",
    "    c: coordinates as SkyCoord object\n",
    "    date: date in astropy Time format\n",
    "    \"\"\"\n",
    "    print(\"Running for %s\" %name)\n",
    "    print(\"Coordinates %s\" %c)\n",
    "    print(\"Date: %s\" %date)\n",
    "\n",
    "    # Find the VLASS tile(s)\n",
    "    tiles = get_tiles()\n",
    "    tilenames, epochs, obsdates = search_tiles(tiles, c)\n",
    "\n",
    "    # Sort the tiles by the epochs so most recent goes last\n",
    "    combined = list(zip(tilenames, epochs, obsdates))\n",
    "    combined_sorted = sorted(combined, key=lambda x: x[1])\n",
    "    tilenames, epochs, obsdates = zip(*combined_sorted)\n",
    "\n",
    "    past_epochs = [\"VLASS1.1v2\", \"VLASS1.2v2\", \"VLASS2.1\", \"VLASS2.2\", \"VLASS3.1\"]\n",
    "    current_epoch = \"VLASS3.2\"\n",
    "\n",
    "    results = []\n",
    "    list_epochs = []\n",
    "    list_dates = []\n",
    "\n",
    "    url_full = 'https://archive-new.nrao.edu/vlass/quicklook/%s/' %(current_epoch)\n",
    "    with closing(urlopen(url_full)) as urlpath:\n",
    "        # Get site HTML coding\n",
    "        string = (urlpath.read().decode('utf-8')).split(\"\\n\")\n",
    "        # clean the HTML elements of trailing and leading whitespace\n",
    "        vals = np.array([val.strip() for val in string])\n",
    "        # Make list of useful html elements\n",
    "        files = np.array(['alt=\"[DIR]\"' in val.strip() for val in string])\n",
    "        useful = vals[files]\n",
    "        # Splice out the name from the link\n",
    "        obsname = np.array([val.split(\"\\\"\")[7] for val in useful])\n",
    "        observed_current_epoch = np.char.replace(obsname, '/', '')\n",
    "\n",
    "    if tilenames[0] is None:\n",
    "        print(\"There is no VLASS tile at this location\")\n",
    "        results = ['images\\\\unimaged.png', 'images\\\\unimaged.png', 'images\\\\unimaged.png']\n",
    "        list_epochs = ['NA', 'NA', 'NA']\n",
    "        list_dates = ['NA', 'NA', 'NA']\n",
    "        return results, list_epochs, list_dates\n",
    "\n",
    "    else:\n",
    "        for ii,tilename in enumerate(tilenames):\n",
    "            start_time = time.time()\n",
    "            print()\n",
    "            print(\"Looking for tile observation for %s\" %tilename)\n",
    "            epoch = epochs[ii]\n",
    "            obsdate = obsdates[ii]\n",
    "            if obsdate not in ['Scheduled', 'Not submitted']:\n",
    "                try:\n",
    "                    obsdate = Time(obsdate, format='iso')\n",
    "                except ValueError as e:\n",
    "                    print(f\"Invalid date format: {obsdate}. Error: {e}\")\n",
    "                    obsdate = 'Invalid date'\n",
    "            else:\n",
    "                obsdate = 'Invalid date'\n",
    "            # Adjust name so it works with the version 2 ones for 1.1 and 1.2\n",
    "            if epoch=='VLASS1.2':\n",
    "                epoch = 'VLASS1.2v2'\n",
    "            elif epoch =='VLASS1.1':\n",
    "                epoch = 'VLASS1.1v2'\n",
    "\n",
    "            observed = True\n",
    "\n",
    "            if epoch not in past_epochs:\n",
    "                if epoch == current_epoch:\n",
    "                    # Check if tile has been observed yet for the current epoch\n",
    "                    if epoch not in observed_current_epoch:\n",
    "                        observed = False\n",
    "                        results.append('images\\\\unimaged.png')\n",
    "                        list_epochs.append(epoch)\n",
    "                        list_dates.append(obsdate)\n",
    "                        print(\"Sorry, tile will be observed later in this epoch\")\n",
    "                else:\n",
    "                    observed = False\n",
    "                    results.append('images\\\\unimaged.png')\n",
    "                    list_epochs.append(epoch)\n",
    "                    list_dates.append(obsdate)\n",
    "                    print(\"Sorry, tile will be observed in a later epoch\")\n",
    "                    \n",
    "            if observed:\n",
    "                subtiles, c_tiles = get_subtiles(tilename, epoch)\n",
    "                if c_tiles == []:\n",
    "                    observed = False\n",
    "                    results.append('images\\\\unimaged.png')\n",
    "                    list_epochs.append(epoch)\n",
    "                    list_dates.append(obsdate)\n",
    "                    print(\"Sorry, tile is not imaged at the position of the source\")\n",
    "                else:\n",
    "                    print(\"Tile Found:\")\n",
    "                    print(tilename, epoch)\n",
    "                    # Find angular separation from the tiles to the location\n",
    "                    dist = c.separation(c_tiles)\n",
    "                    # Find tile with the smallest separation \n",
    "                    subtile = subtiles[np.argmin(dist)]\n",
    "                    url_get = \"https://archive-new.nrao.edu/vlass/quicklook/%s/%s/%s\" %(\n",
    "                            epoch, tilename, subtile)\n",
    "                    imname=\"%s.I.iter1.image.pbcor.tt0.subim.fits\" %subtile[0:-1]\n",
    "                    fname = url_get + imname\n",
    "                    print(fname)\n",
    "                    png_name = \"images\\\\\" + name + \"_\" + epoch + \".png\"\n",
    "                    if os.path.exists(png_name):\n",
    "                        print(f\"PNG file {png_name} already exists. Skipping download.\")\n",
    "                    else:\n",
    "                        # Ensure the new_fits directory exists\n",
    "                        fits_dir = \"new_fits\"\n",
    "                        if not os.path.exists(fits_dir):\n",
    "                            os.makedirs(fits_dir)\n",
    "\n",
    "                        # Define the local path for the downloaded FITS file\n",
    "                        local_fits_path = os.path.join(fits_dir, imname)\n",
    "    \n",
    "                        # Download the FITS file to the new_fits directory\n",
    "                        cmd = f\"curl -o {local_fits_path} {fname}\"\n",
    "                        print(cmd)\n",
    "                        os.system(cmd)\n",
    "    \n",
    "                        # Call get_cutout with the local path of the FITS file\n",
    "                        out = get_cutout(local_fits_path, name, c, epoch)\n",
    "                        out = None\n",
    "\n",
    "                        if out is not None:\n",
    "                            peak, rms, png_name = out\n",
    "                            print(\"Peak flux is %s uJy\" %(peak*1e6))\n",
    "                            print(\"RMS is %s uJy\" %(rms*1e6))\n",
    "                            limit = rms*1e6\n",
    "                            print(\"Tile observed on %s\" %obsdate)\n",
    "                            print(limit, obsdate)\n",
    "                        else:\n",
    "                            png_name = \"images\\\\unimaged.png\"\n",
    "                            print(\"Sorry, tile has not been imaged at the position of the source\")\n",
    "                    # append list elements\n",
    "                    results.append(png_name)\n",
    "                    list_epochs.append(epoch)\n",
    "                    list_dates.append(obsdate)  \n",
    "\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                print(f\"Run search completed in {duration:.2f} seconds.\")\n",
    "\n",
    "        return results, list_epochs, list_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24328\\267207749.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSkyCoord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mra\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxi_ra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxi_dec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrun_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MAXI\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mra_fungi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"16h43m48.201s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_search' is not defined"
     ]
    }
   ],
   "source": [
    "maxi_ra = \"12h10m01.32s\"\n",
    "maxi_dec = \"+49d56m47.006s\"\n",
    "maxi = SkyCoord(ra = maxi_ra, dec = maxi_dec)\n",
    "\n",
    "run_search(\"MAXI\", maxi)\n",
    "\n",
    "ra_fungi = \"16h43m48.201s\"\n",
    "dec_fungi = \"+41d02m43.38s\"\n",
    "first_guy = SkyCoord(ra = ra_fungi, dec = dec_fungi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for SN2018gep\n",
      "Coordinates <SkyCoord (ICRS): (ra, dec) in deg\n",
      "    (250.9508375, 41.04538333)>\n",
      "Date: None\n",
      "\n",
      "Looking for tile observation for T21t17\n",
      "Tile Found:\n",
      "T21t17 VLASS1.2v2\n",
      "https://archive-new.nrao.edu/vlass/quicklook/VLASS1.2v2/T21t17/\n",
      "https://archive-new.nrao.edu/vlass/quicklook/VLASS1.2v2/T21t17/VLASS1.2.ql.T21t17.J164503+413000.10.2048.v1/VLASS1.2.ql.T21t17.J164503+413000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits\n",
      "Generating cutout\n",
      "Cutout centered at position 250.95083749999995, 41.045383333333334\n",
      "PNG Downloaded Successfully\n",
      "Peak flux is 297.11684328503907 uJy\n",
      "RMS is 134.05616850692218 uJy\n",
      "Tile observed on 58607.0\n",
      "134.05616850692218 58607.0\n",
      "\n",
      "Looking for tile observation for T21t17\n",
      "Tile Found:\n",
      "T21t17 VLASS2.2\n",
      "https://archive-new.nrao.edu/vlass/quicklook/VLASS2.2/T21t17/\n",
      "https://archive-new.nrao.edu/vlass/quicklook/VLASS2.2/T21t17/VLASS2.2.ql.T21t17.J164503+413000.10.2048.v1/VLASS2.2.ql.T21t17.J164503+413000.10.2048.v1.I.iter1.image.pbcor.tt0.subim.fits\n",
      "Generating cutout\n",
      "Cutout centered at position 250.95083749999995, 41.045383333333334\n",
      "PNG Downloaded Successfully\n",
      "Peak flux is 241.5875787846744 uJy\n",
      "RMS is 112.8708448274466 uJy\n",
      "Tile observed on 59532.0\n",
      "112.8708448274466 59532.0\n",
      "\n",
      "Looking for tile observation for T21t17\n",
      "Sorry, tile will be observed later in this epoch\n"
     ]
    }
   ],
   "source": [
    "run_search(\"SN2018gep\", first_guy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: vlass_search.py <Name> <RA [deg]> <Dec [deg]> <(optional) Date [astropy Time]>\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__==\"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\\\n",
    "        '''\n",
    "        Searches VLASS for a source.\n",
    "        User needs to supply name, RA (in decimal degrees),\n",
    "        Dec (in decimal degrees), and (optionally) date (in mjd).\n",
    "        If there is a date, then will only return VLASS images taken after that date\n",
    "        (useful for transients with known explosion dates).\n",
    "        \n",
    "        Usage: vlass_search.py <Name> <RA [deg]> <Dec [deg]> <(optional) Date [mjd]>\n",
    "        ''', formatter_class=argparse.RawTextHelpFormatter)\n",
    "        \n",
    "    #Check if correct number of arguments are given\n",
    "    if len(sys.argv) < 3:\n",
    "        print(\"Usage: vlass_search.py <Name> <RA [deg]> <Dec [deg]> <(optional) Date [astropy Time]>\")\n",
    "        sys.exit()\n",
    "     \n",
    "    name = str(sys.argv[1])\n",
    "    ra = float(sys.argv[2])\n",
    "    dec = float(sys.argv[3])\n",
    "    c = SkyCoord(ra, dec, unit='deg')\n",
    "\n",
    "    if glob.glob(\"/Users/annaho/Dropbox/astro/tools/Query_VLASS/VLASS_dyn_summary.php\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Tile summary file is not here. Download it using wget:\\\n",
    "               wget https://archive-new.nrao.edu/vlass/VLASS_dyn_summary.php\")\n",
    "\n",
    "    if (len(sys.argv) > 4):\n",
    "        date = Time(float(sys.argv[4]), format='mjd')\n",
    "        print ('Searching for observations after %s' %date)\n",
    "        run_search(name, c, date) \n",
    "    else:\n",
    "        print ('Searching all obs dates')\n",
    "        run_search(name, c) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
